{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Rayid Ghani, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, Ekaterina Levitskaya, Benjamin Feder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "----\n",
    "In this notebook, we go over these main concepts:\n",
    "- The use of survey weights to get accurate statistics from the sample.\n",
    "- Imputation to account for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "As always, we start by importing any packages we need, as well as creating our connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# database connection\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# statistics package for calculating survey weights\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# regression modeling\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SDR data include survey weights, which allow us to produce estimates for the total population of SEH PhD recipients. In general, survey weights are used because the sample isn't necessarily taken evenly from the population. Sometimes, researchers decide to intentionally oversample from certain subpopulations in order to make sure they have enough people from that group. Re-weighting is also done after the fact to adjust for non-response or other factors that may reduce our sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will compute statistics with and without sampling weights to show how the results differ and demonstrate why using weights is so important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Survey of Doctorate Recipients (SDR)**\n",
    "\n",
    "As in the SDR data we are working with sub-samples of the SED population, we will need to use survey weights in our calculations.\n",
    "\n",
    "Let's find the distribution of earnings for the SED cohort 2015. In the SDR data, we will use the variable `sdryr` (the year of first award of a U.S. PhD degree) to subset by year 2015, and we will also use the `salary` and `wtsurvy` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the relevant variables from the SDR data to find the earnings \n",
    "# among the 2015 cohort\n",
    "\n",
    "query = '''\n",
    "SELECT salary, wtsurvy\n",
    "FROM ncses_2019.nsf_sdr_2017\n",
    "WHERE sdryr = '2015' \n",
    "'''\n",
    "earnings_2015 = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the head of the table\n",
    "\n",
    "earnings_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DescrStatsW` to calculate the weighted earnings distribution. To do this, we essentially give it the variable that we want to calculate statistics for (`salary`), as well as the survey weights it should use (`wtsurvy`). The code below creates a `DescrStatsW` object, from which you obtain various weighted statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_earnings_2015 = DescrStatsW(earnings_2015.salary, weights=earnings_2015.wtsurvy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(weighted_earnings_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the percentiles, we will use a built-in `pandas` function `.quantile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_earnings_2015.quantile([.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we were to ignore the survey weights. You can compare this with the non-weighted estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_2015['salary'].quantile([.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Find the distribution of earnings for 2017</h3></font>\n",
    "\n",
    "Using the `DescrStatsW` function above, find the distribution of earnings for the cohort 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT salary, wtsurvy\n",
    "FROM ncses_2019.nsf_sdr_2017\n",
    "WHERE (INSERT VARIABLE) \n",
    "'''\n",
    "earnings_2017 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first rows of the table\n",
    "earnings_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the weighted estimates\n",
    "# Change the names of the variables\n",
    "\n",
    "weighted_earnings_2017 = DescrStatsW(earnings_2017.VARIABLE, weights=earnings_2017.VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the percentiles\n",
    "weighted_earnings_2017.quantile([.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, it is important to keep in mind that when using survey data (due to the specificities with which every particular survey is designed), the weights always need to be applied in order to be able to draw accurate conclusions about the general population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Replicate Weights\n",
    "\n",
    "The SDR also comes with 104 replicate weights. We won't go into too much detail on what exactly they are, but for those of you who have worked with replicate weights in the past, we will demonstrate how to use them here. \n",
    "\n",
    "We've separated out replicate weights in the data, so we'll need to join the main SDR table and the replicate weights table together to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the relevant variables from the SDR data to find \n",
    "# the earnings among the 2015 cohort\n",
    "\n",
    "query = '''\n",
    "SELECT salary, wtsurvy, rw.*\n",
    "FROM ncses_2019.nsf_sdr_2017 sdr \n",
    "JOIN ncses_2019.nsf_sdr_rw_2017 rw\n",
    "ON sdr.refid = rw.refid\n",
    "WHERE sdryr = '2015'\n",
    "'''\n",
    "earnings_rw_2015 = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the head of the table\n",
    "\n",
    "earnings_rw_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out replicate weights\n",
    "rw_2015 = earnings_rw_2015.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show an example of finding the median salary in the 2015 SDR, along with the variance of the estimate. First, we can find the median using the same method as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_earnings_2015 = DescrStatsW(earnings_rw_2015.salary, weights=earnings_rw_2015.wtsurvy)\n",
    "earnings_median = rw_earnings_2015.quantile(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the variance of this estimate, we can use the formula for using replicate weights as detailed in the SDR 2017 Replicate Weight User Guide (see data documentation on course website for more details). This formula is given by \n",
    "\n",
    "$$ v_{rep}(\\hat{\\theta}) = \\sum^R_{r=1} 0.038461 * (\\hat{\\theta}_r - \\hat{\\theta})^2. $$\n",
    "\n",
    "> The constant term (0.038461) is taken from the Replicate Weight User Guide.\n",
    "\n",
    "To apply this formula quickly and easily, we will use NumPy arrays to do the operation inside the summation all at once, then add them up. The first thing we need to do is create an array of the $\\hat{\\theta}_r$ values, using each set of replicate weights in order to find the estimate 104 times. We'll do this with a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array with NaN first\n",
    "thetas = np.full(104,np.nan)\n",
    "\n",
    "# Loop through and calculate\n",
    "for i in range(104):\n",
    "    thetas[i] = DescrStatsW(earnings_rw_2015.salary, weights=rw_2015.iloc[:,i]).quantile(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use array operations to calculate the variance. That is, for example, if we subtract a scalar from the array, it will do the operation on each element of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_var = np.sum(0.38461 * (thetas - float(earnings_median)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard error\n",
    "np.sqrt(med_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Sometimes, we have variables with missing (or unknown) data. Instead of dropping those values, there are methods to fill those in, in order to be able to use the data. In this example, we will look at an `age_at_dissertation` in the `nsf_sed` table, where we have a lot of missing ages. \n",
    "\n",
    "Keep in mind that these imputed values will be **approximations**, and must be treated as such. If you choose to impute missing values in your project or future work, you must acknowledge your process and clearly state which variables you imputed values for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see how many missing ages we have\n",
    "# Get the count of non-null age at dissertation and count of all rows\n",
    "\n",
    "qry = '''\n",
    "select count(age_at_dissertation) as age_at_diss_count, count(*) as total_count \n",
    "from ncses_2019.nsf_sed;\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to impute the unknown data using the following methods:\n",
    "- Mean imputation;\n",
    "- Regression imputation;\n",
    "- Mode imputation (for categorical variables);\n",
    "- advanced (optional): use machine learning algorithms (such as `K-nearest Neighbors`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. Mean Imputation\n",
    "\n",
    "One of the simplest ways of imputing values is by taking the mean and filling it in. It's possible to do this by using the overall mean, as well as by certain subgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the SED table\n",
    "query = '''\n",
    "SELECT drf_id, age_at_dissertation, phdfield_name, srceprim\n",
    "FROM ncses_2019.nsf_sed\n",
    "'''\n",
    "sed = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the overall mean (could have done this in SQL too)\n",
    "overall_mean = sed.age_at_dissertation.mean() \n",
    "overall_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill the missing values with the value inside `overall_mean`, setting all the missing `age_at_dissertation` values to 34.74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_sed = sed.fillna(overall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this by subgroup. So, for example, we can take the mean in a certain PhD field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_field = sed.groupby('phdfield_name').mean().reset_index()\n",
    "mean_by_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then join this back into the original DataFrame for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Regression Imputation\n",
    "\n",
    "We can also use regression to try to get more accurate values. We build a regression equation from the observations for which we know the age, then use the equation to essentially predict the missing values. This is, in effect, an extension of the mean imputation by subgroup. Here, we will use primary source of support as well as PhD field in order to impute age at dissertation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing rows first\n",
    "sed_nona = sed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the Machine Learning notebooks, we need to use the `get_dummies` function in order to make our categorical variable into dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed_dummied = pd.get_dummies(sed_nona[['phdfield_name','srceprim']], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed_dummied.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model creation process for a linear regression is very similar to that of the ML models when using `scikit-learn`. We create the model object, then give it the data, then use the model object to generate our predictions. The model object essentially contains all of the instructions on how to fit the model, and when we give it the data, it fits the model to that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Predictors and Outcome\n",
    "predictors = sed_dummied\n",
    "outcome = sed_nona.age_at_dissertation\n",
    "\n",
    "# Fit the model\n",
    "ols.fit(X = predictors, y = outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit our model, we can find the predicted values for age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_x = pd.get_dummies(sed.loc[sed.age_at_dissertation.isna(),['phdfield_name','srceprim']], drop_first = True)\n",
    "missing_id = sed.loc[sed.age_at_dissertation.isna(),'drf_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ages = pd.DataFrame({'drf_id':missing_id, 'age_at_dissertation':ols.predict(missing_x)})\n",
    "missing_ages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can join this back in to our original dataset in order to get our complete data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint: Use another variable for a regression</h3></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of another variable that can be used in the regression to predict the age at dissertation. Include it in the model, and try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation for Categorical Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute using mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical variables, it doesn't make sense to try something like mean imputation, because there isn't a mean to calculate. Another method of filling in the missing values is to find the most frequent value (mode), and impute using that mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `groupby` function to find the frequency of payment per household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the marital status and age\n",
    "query = '''\n",
    "SELECT marital, age_at_dissertation\n",
    "FROM ncses_2019.nsf_sed\n",
    "'''\n",
    "marital_status = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the values of `marital` to `None` so that it is properly treated as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_status.loc[marital_status['marital'] == 'NA','marital'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `groupby` function to find the frequency of marital status categories per age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_count = marital_status.groupby(['age_at_dissertation','marital'])['marital'].count().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the most frequent marital status values per age\n",
    "marital_mode = marital_count.sort_values('count', ascending=False).drop_duplicates(['age_at_dissertation'])\n",
    "\n",
    "marital_mode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now drop the `count` column\n",
    "marital_mode = marital_mode.drop('count',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Marital_mode` is now our lookup table with the most frequent value of a marital status per age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the original table with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table with missing values\n",
    "query = '''\n",
    "SELECT marital, age_at_dissertation\n",
    "FROM ncses_2019.nsf_sed\n",
    "'''\n",
    "marital_status = pd.read_sql(query, conn)\n",
    "\n",
    "# Replace NA with None object\n",
    "marital_status.loc[marital_status['marital'] == 'NA','marital'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge the original table (`marital status`) with the look up table. This will create two columns: `marital_x` (original marital status value) and `marital_y` (marital status value from the lookup table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = marital_status.merge(marital_mode, on='age_at_dissertation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one age (`25`) as an example. We can see the missing values in the `marital_x` column. We are going to replace them by values in the `marital_y` column (mode that we found above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['age_at_dissertation'] == 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the None values in the `marital_x` column with the known values in the column `marital_y`\n",
    "merged.loc[merged['marital_x'].isnull(), 'marital_x'] = merged['marital_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the `mop_y` column\n",
    "merged = merged.drop('marital_y',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check at the results for the same age as we looked at before. The None values are replaced with the most frequent marital status value for that age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['age_at_dissertation'] == 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Advanced: Using machine learning to impute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute values, we can also use the machine learning algorithm called the `K-nearest Neighbors`. The principle behind it is quite simple: the missing values can be imputed by values of \"closest neighbors\" - as approximated by other, known, features. \n",
    "\n",
    "For example, if we had cases where the data on a marital status was completely missing per age, we could approximate their marital status by referring to other characteristics which could be shared by that age group (their 'closest neighbors' in terms of characteristics).\n",
    "\n",
    "The algorithm calculates the distance between the input values (the missing values) and helps to identify the nearest possible value based on other features (such as known characteristics of the closest age group)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
