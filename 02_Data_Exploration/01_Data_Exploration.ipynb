{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Brian Kim, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Avishek Kumar, Jonathan Morgan, Ekaterina Levitskaya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). We'd also have perfect data documentation, with summary statistics and approproiate aggregate measures of everything we'd want to investigate. However, that is hardly ever true - and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF, and we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. Throughout the notebook, we will demonstrate various techniques of how to use SQL and Python to explore the various datasets that we have and better understand what we are working with. This will form the basis of all the other types of analyses we do in this class and is a crucial first step for any data analysis workflow. As you work through the notebook, we will have checkpoints for you try out your own code, but you can also think about how you might apply any of the techniques and code presented with other datasets as well. What we are showing is just a portion of what you might be interested in investigating, so don't feel restricted by the questions we've decided to try to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets We Will Explore In This Notebook**\n",
    "- **Survey of Earned Doctorates (SED)**: individual level data (educational history, demographic, and postgraduation plans) of individuals receiving research doctoral degrees from U.S. academic institutions.\n",
    "- **Survey of Doctorate Recipients (SDR)**: individual level data (demographic, education, and career history information from individuals with a U.S. research doctoral degree in a science, engineering, or health (SEH) field).\n",
    "- **UMETRICS**: institution/individual level administrative universities' data.\n",
    "- **HERD**: institution level data on R&D funding.\n",
    "\n",
    "You will have an opportunity to explore the different datasets in the ADRF, and this notebook will take you around the different ways you can analyze your data. This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values, and so on. \n",
    "\n",
    "This will be done using both SQL and `pandas` in Python. The `sqlalchemy` Python package will give you the opportunity to interact with the database using SQL to pull data into Python. Some additional manipulations will be handled by Pandas in Python (by converting your datasets into dataframes).\n",
    "\n",
    "**This notebook will provide an introduction and examples for:**\n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to explore aggregate metrics\n",
    "- How to join tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our class database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- subsetting data\n",
    "- `groupby`\n",
    "- `merge`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- select data subsets\n",
    "- sum over groups\n",
    "- create new tables\n",
    "- count distinct values of desired variables\n",
    "- order data by chosen variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Among the most famous Python packages:\n",
    "- **numpy** is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- **pandas** is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- **sqlalchemy** is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are working with the survey data, we will also use the **statsmodels** package in order to be able to apply survey weights in our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy \n",
    "import numpy as np\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy\n",
    "\n",
    "# get weighted estimates\n",
    "from statsmodels.stats.weightstats import DescrStatsW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method by placing a cursor near the name of the method and pressing shift + tab.__\n",
    "\n",
    "__The `help()` function also provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and pandas in particular - make it much easier to calculate descriptive statistics and perform more complicated analyses with the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use pandas to read data from a relational database. For examples to read data from a CSV file, refert to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a SQL query from pgAdmin, this function will ask for some information about the database, and what query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection we will use the SQLAlchemy package and tell it which database we want to connect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (eg when building queries), some examples are:\n",
    "1. Empty brackets (shown above) which simply inserts the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries eg when creating \"labels\" and \"features\" for Machine Learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "\n",
    "This part is similar to writing a SQL query in DBeaver. Depending on what data we are interested in, we can use different queries to pull different data. In this example, we will pull all the content from the SED data for doctoral students who graduated in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "LIMIT 10\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the `LIMIT` provides one simple way to get a \"sample\" of data; however, using `LIMIT` does **not provide a _random_** sample. You may get different samples of data than others using just the `LIMIT` clause, but it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function, and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function and assign the variable `df`\n",
    "# to the dataframe returned by the function\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schemas and Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a reminder, in this class you have access to the following schemas: `ncses_2019` and `ada_ncses_2019`. You only have write privileges to the `ada_ncses_2019` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname = 'ncses_2019'\n",
    "'''\n",
    "tables = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tables['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, take some time to look at the documentation and understand what the different variables refer to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Read in the table with SDR data</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the code above, read in and explore the table with SDR 2017 data (name: `nsf_sdr_2017`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM ncses_2019.nsf_sdr_2017\n",
    "'''\n",
    "sdr_2017 = pd.read_sql(query,conn)\n",
    "\n",
    "sdr_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "In this section, we will start looking at aggregate statistics on the data. The goal of this exercise is to get a better understanding of the data we working with. As you work through this section, try to ask yourself some questions: Are the data generally clean? What are possible sources of error? What are the types of objects and variables that you are working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __Large tables__ can take a long time to process on shared databases, so we will demonstrate using SQL and Python with consideration for how much data we are reading back into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer these broader research questions, let's start by looking at simple aggregate statistics in each of our data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration #1: **Survey of Earned Doctorates (SED)**\n",
    "\n",
    "**Motivating Question:** What is a primary source of funding for doctorate students?\n",
    "\n",
    "In order to avoid pulling a large amount of information that we don't need to answer the question, let's only\n",
    "pull in the data with the unique identifier of a person (`drf_id`) and their primary source of support (in the SED data this variable is called `srceprim`, primary source of support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query and select only two variables: unique identifier (drfid) and primary source of support (srceprim)\n",
    "\n",
    "query = '''\n",
    "SELECT drf_id, srceprim\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it into a pandas dataframe\n",
    "\n",
    "sed_ncses_2015 = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first rows of the table\n",
    "\n",
    "sed_ncses_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what are the unique values in our primary support variable. We can use `SELECT DISTINCT` in SQl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT(srceprim)\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "'''\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `COUNT`, `GROUP BY` and `ORDER BY` functions in SQL to aggregate the number of graduates in each category and sort them in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of graduates (their unique identifiers), group by a primary source of support variable, and sort \n",
    "# the counts in a descending order\n",
    "\n",
    "query = '''\n",
    "SELECT COUNT(drf_id), srceprim\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "GROUP BY srceprim\n",
    "ORDER BY COUNT(drf_id) DESC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_support = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_support  # call the name of the dataframe to view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to see in the same table what those categories stand for - here we looked up the categories and created a separate dataframe, with which we will merge our dataframe above on the column with category letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with description of primary support categories\n",
    "\n",
    "primary_source = pd.DataFrame()\n",
    "primary_source['srceprim'] = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N']\n",
    "primary_source['description'] = ['Fellowship, scholarship', 'Dissertation grant', 'Teaching assistantship', 'Research assistantship',\n",
    "                                'Other assistantship','Traineeship','Internship, clinical residency', 'Loans (from any source)',\n",
    "                                'Personal savings', 'Personal earnings during graduate school (other than sources listed above)',\n",
    "                                'Spouse\\'s, partner\\'s, or family\\'s earnings or savings', 'Employer reimbursement/assistance',\n",
    "                                'Foreign (non-U.S.) support', 'Other - specify']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `pandas` method `.merge()` to join two tables, specifying which column we want to combine the two DataFrame objects on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_merged = primary_source.merge(primary_support,on='srceprim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also sort the values, with the largest value first - for that, we use `.sort_values()` function and specify a parameter `ascending=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted = primary_source_merged.sort_values('count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to know a percentage of the cohort? To calculate that, we can get the sum of the column with number of graduates to find out the total number, using function `.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted['count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new column with this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted['total_cohort'] = [48419] * len(primary_source_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` allows for a very quick and easy calculation of columns in a dataframe - to find out the percentage, we will simply divide the column with the number of graduates by the total number of graduates and create a new column called `percentage` with those values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted['percentage'] = primary_source_sorted['count'] / primary_source_sorted['total_cohort'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_source_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 2: Find a secondary source of support</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same cohort of 2015, repeat the code above and find a secondary source of support (variable: `srcesec`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variable of interest, count and group by a category for 2015 cohort and sort values\n",
    "\n",
    "query = '''\n",
    "SELECT COUNT(drf_id), srcesec\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "GROUP BY srcesec\n",
    "ORDER BY COUNT(drf_id) DESC\n",
    "'''\n",
    "\n",
    "secondary_support = pd.read_sql(query,conn)\n",
    "\n",
    "secondary_support.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration #2: **UMETRICS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivating Question:** How important is federal funding for doctorate recipients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMETRICS data can help provide insight into the funding history of doctorate recipients. In the `semester` file, we can see a source which most frequently funds a given student in a given semester (variable `modal_funder`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the available information on funding history for the SED cohort 2015, we will need to join the tables. We'll do this using SQL code, and bring in the joined table into Python as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we join the SED table using an SED-UMETRICS crosswalk, and then we join the resulting table with the IRIS semester table\n",
    "# get get the name of the most frequently funding agency and the number of semesters \n",
    "query = '''\n",
    "SELECT sed.drf_id, iris_semester.modal_funder, COUNT(iris_semester.semester) AS \"number_semesters\"\n",
    "FROM ncses_2019.nsf_sed sed\n",
    "JOIN ncses_2019.sed_umetrics_xwalk xwalk ON sed.drf_id = xwalk.drf_id\n",
    "JOIN ncses_2019.iris_semester iris_semester ON iris_semester.emp_number = xwalk.emp_number\n",
    "WHERE sed.phdfy = '2015'\n",
    "GROUP BY sed.drf_id, iris_semester.modal_funder\n",
    "'''\n",
    "funding = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this information with the primary source of support variable which we explored above in the SED dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT drf_id, srceprim\n",
    "FROM ncses_2019.nsf_sed\n",
    "WHERE phdfy = '2015'\n",
    "'''\n",
    "source_support_sed = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_support_sed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the table defined above with categories' definitions\n",
    "source_support = source_support_sed.merge(primary_source,on='srceprim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_support.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge with the IRIS data on funding defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_comparison = source_support.merge(funding,on='drf_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_comparison.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset by an individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_comparison[funding_comparison['drf_id'] == 'XXX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to a joined SED-UMETRICS dataset, we now have a more detailed picture of a person's funding history. DELETED DURING DISCLOSURE REVIEW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 3: Find number of semesters by non-federal source of funding.</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRIS `semester` file has a flag for non-federal sources of funding called `any_non_federal`. Find the number of semesters where `any_non_federal` source of funding is True (equals 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM ncses_2019.iris_semester\n",
    "WHERE any_non_federal = 1\n",
    "'''\n",
    "\n",
    "non_federal_funding = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federal_funding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration #3: **Survey of Doctorate Recipients (SDR)**\n",
    "\n",
    "**Motivating Question:** What is the distribution of earnings by gender and by race/ethnicity?\n",
    "\n",
    "As in the SDR data we are working with sub-samples of the SED population, we will need to use survey weights in our calculations.\n",
    "\n",
    "Let's find the distribution of earnings for the SED cohort 2015. In the SDR data, we will use the variable `sdryr` (the year of first award of a U.S. PhD degree) to subset by year 2015, and we will also use `salary`, `gender`, and `wtsurvy` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the relevant variables from the SDR data to find the female earnings among the 2015 cohort\n",
    "\n",
    "query = '''\n",
    "SELECT salary, wtsurvy\n",
    "FROM ncses_2019.nsf_sdr_2017\n",
    "WHERE sdryr = '2015' \n",
    "AND gender = 'F'\n",
    "'''\n",
    "female_earnings = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_earnings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the `DescrStatsW` function to calculate the weighted earnings distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_female_earnings = DescrStatsW(female_earnings.salary, weights=female_earnings.wtsurvy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the percentiles, we will use a built-in `pandas` function `.quantile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_female_earnings.quantile([.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 4: Find the distribution of earnings by race/ethnicity</h3></font>\n",
    "\n",
    "Using the `DescrStatsW` function above, find the distribution of earnings for the Hispanic population for the cohort 2015 (variable `racethm = '4'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT salary, wtsurvy\n",
    "FROM ncses_2019.nsf_sdr_2017\n",
    "WHERE sdryr = '2015' \n",
    "AND racethm = '4'\n",
    "'''\n",
    "earnings_hispanic = pd.read_sql(query,conn)\n",
    "\n",
    "earnings_hispanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the weighted estimates\n",
    "weighted_earnings_hispanic = DescrStatsW(earnings_hispanic.salary, weights=earnings_hispanic.wtsurvy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the percentiles\n",
    "weighted_earnings_hispanic.quantile([.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration #4: **Higher Education Research and Development Survey (HERD)**\n",
    "\n",
    "**Motivating Question:** What are the institutional characteristics of the various schools from which graduate students receive their PhDs?\n",
    "\n",
    "To answer this question, let's explore the HERD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM ncses_2019.nsf_herd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "herd = pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "herd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HERD data has flags for whether the university has a medical school. The values are stored as `boolean`:\n",
    "    `True` or `False` (for whether a university has a medical school)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of universities with medical school:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT COUNT(std_inst_name)\n",
    "FROM ncses_2019.nsf_herd\n",
    "WHERE med_sch_flag = 'T'\n",
    "'''\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same in `pandas` by subsetting the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_school_flag = herd[herd['med_sch_flag'] == 'T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And counting the total number of rows of universities with a medical school by calling `len` (length of a dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(herd[herd['med_sch_flag'] == 'T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 4: Explore the HERD data</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find how many universities are flagged as historically black colleges and universities (variable: `hbcu_flag`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT COUNT(std_inst_name)\n",
    "FROM ncses_2019.nsf_herd\n",
    "WHERE hbcu_flag = 'T'\n",
    "'''\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Order by total R&D funding in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT COUNT(std_inst_name)\n",
    "FROM ncses_2019.nsf_herd\n",
    "GROUP BY srcesec\n",
    "ORDER BY COUNT(drf_id) DESC\n",
    "'''\n",
    "pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reminder: you can refer to the documentation for more information on each dataset"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
