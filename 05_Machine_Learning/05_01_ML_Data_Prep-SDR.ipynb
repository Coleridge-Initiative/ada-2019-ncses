{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"./images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \n",
    "\n",
    "_Citation to be updated on export_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go over the data preparation process for setting up our tables for Machine Learning. The next notebook will discuss actually applying the machine learning methods as well as the evaluation process. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "We want to use characteristics about individuals based on their answers in the SED as well as characteristics about their institution based on the HERD in order to predict whether a graduate student goes into academia after receiving their doctorate. More specifically, we will base the outcome on the answers to the SDR two years afterwards, which limits the doctorate recipients that we predict on to scientists and engineers. That is, our main question of interest is:\n",
    "\n",
    "> **Which science, engineering, and health students will go into academia upon receiving their PhD?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `sqlalchemy` from previous tutorials. We'll be using these same tools, as well as many SQL queries, in order to prepare our data for our Machine Learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# We will use weighted statistics\n",
    "from statsmodels.stats.weightstats import DescrStatsW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set database connections\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** In this notebook, we create a series of tables to use later on. In order to avoid having everyone in the class running this notebook and trying to simultaneously create the same tables from the same sources, we have used conditional statements to prevent creating the tables if they already exist. When you adapt this code for your projects, make sure you are creating the table, and that you are creating them in the appropriate schema (`ada_ncses_2019`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT * \n",
    "FROM pg_tables\n",
    "WHERE schemaname = 'ada_ncses_2019'\n",
    "'''\n",
    "\n",
    "# List of tables inside ada_ncses_2019\n",
    "tables = pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels\n",
    "\n",
    "Labels are the dependent variables, or y variables, that we are trying to predict. In the machine learning framework, your labels are usually *binary*: true or false, often encoded as 1 or 0. We start by defining a cohort of people to predict on as well as their associated labels. This is what we'll use as our basis for adding on any features, or predictor variables.\n",
    "\n",
    "It is important to clearly and explicitly define the rows (aka observations) of your analysis to ensure you properly combine input datasets and populate the columns (aka features).\n",
    "\n",
    "For this example, we will consider for our training set the cohort of graduate students who graduated with their doctorate in the 2012-2013 academic year and was part of the 2015 SDR. The testing set will be the cohort of graduate students who graduated in 2014-2015 and answered the 2017 SDR. Don't worry too much about how the training and testing sets work for now; we'll cover this in more detail in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome: Predict whether a graduate is in academia two years after receiving their PhD.\n",
    "\n",
    "Our training cohort consists of 2012-2013 academic year graduates, while the testing cohort consists of 2014-2015 academic year graduates. We will now create a `label` variable that is set to `1` if a person went into academia after earning their doctorate and `0` if not. This will, at the same time, define the cohort of people for our training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.sdr_label_2013;\n",
    "CREATE TABLE ada_ncses_2019.sdr_label_2013 AS\n",
    "SELECT sed.drf_id, sdr.refid, sed.phdinst, wtsurvy,\n",
    "    CASE WHEN (edtp != '1' and edtp != 'L') THEN 1 ELSE 0 END as label\n",
    "FROM ncses_2019.nsf_sed sed\n",
    "JOIN ncses_2019.sdr_drfid_2015 xwalk\n",
    "ON sed.drf_id = xwalk.drf_id\n",
    "JOIN ncses_2019.nsf_sdr_2015 sdr\n",
    "ON xwalk.refid = sdr.refid\n",
    "WHERE sed.phdfy = '2013'\n",
    "\"\"\"\n",
    "\n",
    "if 'sdr_label_2013' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a JOIN with the SED and SDR data, since we want to take the outcome from the SDR. Note that we are using the `CASE WHEN` statement here. This gives us a way to create a new binary variable, setting it equal to 1 when `edtp` is not one of the codes that are associated with a non-academia job, and 0 otherwise. We do the same with the 2015 SED cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.sdr_label_2015;\n",
    "CREATE TABLE ada_ncses_2019.sdr_label_2015 AS\n",
    "SELECT sed.drf_id, sdr.refid, sed.phdinst, wtsurvy,\n",
    "    CASE WHEN (edtp != '1' and edtp != 'L') THEN 1 ELSE 0 END as label\n",
    "FROM ncses_2019.nsf_sed sed\n",
    "JOIN ncses_2019.sdr_drfid_2017 xwalk\n",
    "ON sed.drf_id = xwalk.drf_id\n",
    "JOIN ncses_2019.nsf_sdr_2017 sdr\n",
    "ON xwalk.refid = sdr.refid\n",
    "WHERE sed.phdfy = '2015'\n",
    "\"\"\"\n",
    "\n",
    "if 'sdr_label_2015' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created two label tables, let's take a look at them to see if they seem to be what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM ada_ncses_2019.sdr_label_2013\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the balance in our label. This is important for later, because this will provide the basis for our random model baseline in the evaluation portion of the machine learning process. \n",
    "\n",
    "Since the SDR uses survey weights, we make sure to use the weights in calculating our proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtstats = DescrStatsW(df.label, weights = df.wtsurvy)\n",
    "wtstats.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what the actual values are in the dataset using the `crosstab` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df['label'], columns =  'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for our 2015 cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM ada_ncses_2019.sdr_label_2015\", conn)\n",
    "wtstats = DescrStatsW(df.label, weights = df.wtsurvy)\n",
    "wtstats.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though between around XX% and around XX% of doctoral recipients in science, engineering, and health go into academia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Create a label table</h3></font>\n",
    "\n",
    "Try creating a different label table based on a slightly different definition of the label. How would you create the labels if you were interested in whether graduates went into a government job? What if you wanted a different cohort (for example, only looking at people in certain fields)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "SELECT sed.drf_id, sdr.refid, sed.phdinst, wtsurvy,\n",
    "    CASE WHEN (sdr.emsecsm = '1' and sdr.emsecsm != 'L') THEN 1 ELSE 0 END as label\n",
    "FROM ncses_2019.nsf_sed sed\n",
    "JOIN ncses_2019.sdr_drfid_2015 xwalk\n",
    "ON sed.drf_id = xwalk.drf_id\n",
    "JOIN ncses_2019.nsf_sdr_2015 sdr\n",
    "ON xwalk.refid = sdr.refid\n",
    "WHERE sed.phdfy = '2013'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the SQL code into pandas\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many people get a label = 1 (working at the education institution)\n",
    "len(df[df['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who work in business (change the variable to `emsecsm = '3'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "SELECT sed.drf_id, sdr.refid, sed.phdinst, wtsurvy,\n",
    "    CASE WHEN (sdr.emsecsm = '3' and sdr.emsecsm != 'L') THEN 1 ELSE 0 END as label\n",
    "FROM ncses_2019.nsf_sed sed\n",
    "JOIN ncses_2019.sdr_drfid_2015 xwalk\n",
    "ON sed.drf_id = xwalk.drf_id\n",
    "JOIN ncses_2019.nsf_sdr_2015 sdr\n",
    "ON xwalk.refid = sdr.refid\n",
    "WHERE sed.phdfy = '2013'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the SQL code into pandas\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many people get a label = 1 (working in business)\n",
    "len(df[df['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem. \n",
    "\n",
    "- **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "\n",
    "Example of feature engineering are: \n",
    "\n",
    "- **Transformations**, such a log, square, and square root.\n",
    "- **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables\n",
    "(such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "- **Discretization**. Several methods require features to be discrete instead of continuous. This is often done \n",
    "by binning, which you can do by various approaches like equal width, deciles, Fisher-Jenks, etc. \n",
    "- **Aggregation** Aggregate features often constitute the majority of features for a given problem. These use \n",
    "different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several\n",
    "values into one feature, aggregating over varying windows of time and space. For example, for policing or criminal justice problems, we may want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    ">This notebook walks through creating the following features:\n",
    ">- Individual-level characteristics, taken from the SED.\n",
    ">- Institution-level characteristics from HERD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation Plan\n",
    "\n",
    "We will be creating a series of temporary tables containing all of the features we want to include in our model. These tables will be:\n",
    "- `features_ind`: This table will contain information from the SED about the individual, such as source of funding, race, years worked on dissertation, etc.\n",
    "- `features_inst`: This table will contain information about the institution.\n",
    "\n",
    "We will then join these tables together, along with our labels table, to create the full dataset that we use for our machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual level features\n",
    "\n",
    "We will start by creating a table containing all of the individual level features. In general, this is mostly information taken from the SED.\n",
    "\n",
    "For many variables, we can simply use them as they are. However, `sklearn`, which we will be using for our machine learning algorithms, we need to convert all categorical variables into binary variables (that is, make dummy variables). We can do that in Python, so it will be covered in the next notebook. \n",
    "\n",
    "We'll start by first bringing in the variables that are in the SED.\n",
    "\n",
    "> The list of variables we're including isn't exhaustive, or even what you might be interested in looking at. These are just sample features, and you should think carefully about what you're interested in and what to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SED Individual variables\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_ind_2013;\n",
    "CREATE TEMP TABLE features_ind_2013 AS \n",
    "SELECT cohort.drf_id, cohort.label, -- ID and Label\n",
    "doccode, tuitrems, srceprim, srce1ed, srcesec, srcea, srceb, srcec, srced, srcef, srceg, srceh, srcei, srcej, srcek, srcel, srcem, srcen, -- funding\n",
    "udebtlvl, gdebtlvl, -- debt info\n",
    "phdfield_name, --phd info\n",
    "race, sex, -- demographic variables \n",
    "age_at_dissertation, -- age at time of doctorate\n",
    "yrscours, yrsdisst,yrsnotwrk, -- PhD Program workload\n",
    "wtsurvy -- survey weight\n",
    "FROM ada_ncses_2019.sdr_label_2013 cohort\n",
    "LEFT JOIN ncses_2019.nsf_sed sed\n",
    "ON cohort.drf_id = sed.drf_id\n",
    "where phdfy = '2013';\n",
    "'''\n",
    "conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SED Individual variables\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_ind_2015;\n",
    "CREATE TEMP TABLE features_ind_2015 AS \n",
    "SELECT cohort.drf_id, cohort.label, -- ID and Label\n",
    "doccode, tuitrems, srceprim, srce1ed, srcesec, srcea, srceb, srcec, srced, srcef, srceg, srceh, srcei, srcej, srcek, srcel, srcem, srcen, -- funding\n",
    "udebtlvl, gdebtlvl, -- debt info\n",
    "phdfield_name, --phd info\n",
    "race, sex, -- demographic variables \n",
    "age_at_dissertation, -- age at time of doctorate\n",
    "yrscours, yrsdisst,yrsnotwrk, -- PhD Program workload\n",
    "wtsurvy -- survey weight\n",
    "FROM ada_ncses_2019.sdr_label_2015 cohort\n",
    "LEFT JOIN ncses_2019.nsf_sed sed\n",
    "ON cohort.drf_id = sed.drf_id\n",
    "where phdfy = '2015';\n",
    "'''\n",
    "conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data to make sure it's working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_ind_2013', conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_ind_2015', conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in Institutional characteristics\n",
    "\n",
    "Now, we want to add in institutional characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in institutional level features \n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_inst_2013;\n",
    "CREATE TEMP TABLE features_inst_2013 AS\n",
    "SELECT cohort.drf_id, cohort.phdinst,\n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM ada_ncses_2019.sdr_label_2013 cohort\n",
    "LEFT JOIN ncses_2019.nsf_herd herd\n",
    "ON cohort.phdinst = herd.ipeds_inst_id \n",
    "where herd.year = '2013'\n",
    "'''\n",
    "\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in institutional level features \n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_inst_2015;\n",
    "CREATE TEMP TABLE features_inst_2015 AS\n",
    "SELECT cohort.drf_id, cohort.phdinst, \n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM ada_ncses_2019.sdr_label_2015 cohort\n",
    "LEFT JOIN ncses_2019.nsf_herd herd\n",
    "ON cohort.phdinst = herd.ipeds_inst_id\n",
    "where herd.year = '2015'\n",
    "'''\n",
    "\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll do a little checking to make sure the variables we're bringing in look like they should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_inst_2015', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the `isna()` method for DataFrames in order to check if there are missing values. We use it in conjunction with the `sum()` method to find how many missing values there are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all to make a features table\n",
    "\n",
    "Now that we've created all of our individual and institution level feature tables, we can combine them all into one big table that we will use for our machine learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.sdr_ml_2013;\n",
    "CREATE TABLE ada_ncses_2019.sdr_ml_2013 AS\n",
    "SELECT ind.drf_id, label, wtsurvy, -- ID and label\n",
    "tuitrems, srceprim, srce1ed, srcesec, srcea, srceb, srcec, srced, srcef, srceg, srceh, srcei, srcej, srcek, srcel, srcem, srcen, -- funding\n",
    "udebtlvl, gdebtlvl, -- debt info\n",
    "phdfield_name, --phd info\n",
    "race, sex, -- race variables \n",
    "age_at_dissertation, -- age at time of doctorate\n",
    "yrscours, yrsdisst,yrsnotwrk, -- PhD Program workload\n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM features_ind_2013 ind\n",
    "LEFT JOIN features_inst_2013 inst\n",
    "ON ind.drf_id = inst.drf_id\n",
    "'''\n",
    "\n",
    "if 'sdr_ml_2013' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.sdr_ml_2015;\n",
    "CREATE TABLE ada_ncses_2019.sdr_ml_2015 AS\n",
    "SELECT ind.drf_id, label, wtsurvy, -- ID and label\n",
    "tuitrems, srceprim, srce1ed, srcesec, srcea, srceb, srcec, srced, srcef, srceg, srceh, srcei, srcej, srcek, srcel, srcem, srcen, -- funding\n",
    "udebtlvl, gdebtlvl, -- debt info\n",
    "phdfield_name, --phd info\n",
    "race, sex, -- race variables \n",
    "age_at_dissertation, -- age at time of doctorate\n",
    "yrscours, yrsdisst,yrsnotwrk, -- PhD Program workload\n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM features_ind_2015 ind\n",
    "LEFT JOIN features_inst_2015 inst\n",
    "ON ind.drf_id = inst.drf_id\n",
    "'''\n",
    "\n",
    "if 'sdr_ml_2015' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from ada_ncses_2019.sdr_ml_2015', conn)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3>Checkpoint 2: Create a Feature and add to the feature table</h3></font>\n",
    "\n",
    "What are some additional features you might want to add? Think about the different variables in all of the different tables that you have access to, both at the institutional level and at the individual level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `race2` variable\n",
    "sql = '''\n",
    "SELECT cohort.drf_id, cohort.label, -- ID and Label\n",
    "race2 -- detailed ethnicity code\n",
    "FROM ada_ncses_2019.sdr_label_2015 cohort\n",
    "LEFT JOIN ncses_2019.nsf_sed sed\n",
    "ON cohort.drf_id = sed.drf_id\n",
    "where phdfy = '2015';\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into a pandas dataframe\n",
    "race2 = pd.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the existing table with features\n",
    "added_features = df.merge(race2, on=['drf_id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the new variable has been added to the columns list\n",
    "added_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Considerations\n",
    "\n",
    "Notice that there are missing values in the final table we created. We'll have to figure out how to deal with those. By default, if we try to run our machine learning models using scikit-learn, it will use listwise deletion, which is not always desirable. In addition, we should carefully consider whether there are any other errors in our dataset. For example, there might have been data entry errors, or coding mistakes when transferring the data.\n",
    "\n",
    "### Removing Outliers \n",
    "\n",
    "**It is never a good idea to drop observations without prior investigation AND a good reason to believe the data is wrong!** \n",
    "\n",
    "### Imputing Missing Values\n",
    "\n",
    "There are many ways of imputing missing values based on the rest of the data. Missing values can be imputed to median of the rest of the data, or you can use other characteristics (eg industry, geography, etc.).\n",
    "\n",
    "For our data, we have made an assumption about what \"missing\" means for each of our data's components (eg if the individual does not show up in the IDES data we say they do not have a job in that time period).\n",
    "\n",
    "Before running any machine learning algorithms, we have to ensure there are no `NULL` (or `NaN`) values in the data for both our testing and training sets. As you have heard before, __never remove observations with missing values without considering the data you are dropping__. One easy way to check if there are any missing values with `Pandas` is to use the `.info()` method, which returns a count of non-null values for each column in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
