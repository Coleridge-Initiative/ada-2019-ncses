{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"./images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \n",
    "\n",
    "_Citation to be updated on export_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go over the data preparation process for setting up our tables for Machine Learning. The next notebook will discuss actually applying the machine learning methods as well as the evaluation process. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "We want to use information about an individual's funding history combined with characteristics about individuals based on their answers in the SED as well as characteristics about their institution based on the HERD, PatentsView, and Federal Reporter in order to predict whether a graduate student goes into academia after receiving their doctorate. More specifically, we will base the outcome on the answers to the post-graduation plan questions in SED to answer the question:\n",
    "\n",
    "> **Of students who graduated from an IRIS member institution, who will go into academia upon receiving their PhD?**\n",
    "\n",
    "Note that because we only have UMETRICS funding data from IRIS member institutions, we can only generalize to that population. If we try to generalize the results to be about PhD graduates in general, we are making an additional assumption that the IRIS member institutions are essentially the same as non-member institutions in their behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `sqlalchemy` from previous tutorials. We'll be using these same tools, as well as many SQL queries, in order to prepare our data for our Machine Learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set database connections\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** In this notebook, we create a series of tables to use later on. In order to avoid having everyone in the class running this notebook and trying to simultaneously create the same tables from the same sources, we have used conditional statements to prevent creating the tables if they already exist. When you adapt this code for your projects, make sure you are creating the table, and that you are creating them in the appropriate schema (`ada_ncses_2019`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT * \n",
    "FROM pg_tables\n",
    "WHERE schemaname = 'ada_ncses_2019'\n",
    "'''\n",
    "\n",
    "tables = pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels\n",
    "\n",
    "Labels are the dependent variables, or y variables, that we are trying to predict. In the machine learning framework, your labels are usually *binary*: true or false, often encoded as 1 or 0. We start by defining a cohort of people to predict on as well as their associated labels. This is what we'll use as our basis for adding on any features, or predictor variables.\n",
    "\n",
    "It is important to clearly and explicitly define the rows (aka observations) of your analysis to ensure you properly combine input datasets and populate the columns (aka features).\n",
    "\n",
    "For this example, we want to use the funding information from the UMETRICS data, along with some basic demographic information and institutional characteristics, in order to predict whether a PhD recipient goes into academia or not. Since we want to maximize the amount of data that we have and our main limiting factor is the number of IRIS member universities, we chose to limit our analytical frame to just 2014-2015 graduates from IRIS member universities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome: Predict whether an IRIS member university graduate goes into academia upon receiving their PhD.\n",
    "\n",
    "We will now create a `label` variable that is set to `1` if a person went into academia after earning their doctorate and `0` if not. We will keep our full dataset together for now, and separate into training and testing sets in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.umetrics_label;\n",
    "CREATE TABLE ada_ncses_2019.umetrics_label AS\n",
    "SELECT drf_id, phdinst,\n",
    "    CASE WHEN pdemploy IN ('A', 'B', 'C', 'D') THEN 1 ELSE 0 END as label  \n",
    "FROM ncses_2019.nsf_sed \n",
    "WHERE phdfy = '2015' \n",
    "AND \n",
    "phdinst in -- list IRIS member universities\n",
    "    ('List of University IDs that cannot be disclosed');\n",
    "\"\"\"\n",
    "\n",
    "if 'umetrics_label' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a JOIN with the SED and UMETRICS crosswalk, since we want to take the outcome from the SED, but only want to include the people who are from UMETRICS member institutions. Recall that we used this list of IRIS member institutions in the Visualizations notebook for the funding history diagram, because these were the only ones with information for 2012 to 2015. We're able to then just keep the rows in the SED that correspond to people whose `phdinst` are in that list of member universities as well as graduated in 2015.\n",
    "\n",
    "Note that we are using the `CASE WHEN` statement here. This gives us a way to create a new binary variable, setting it equal to 1 when `pdemploy` is one of the codes that are associated with an academia job, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the label table, let's take a look to see if it seems to be what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM ada_ncses_2019.umetrics_label\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the balance in our label. This is important for later, because this will provide the basis for our random model baseline in the evaluation portion of the machine learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df['label'], columns =  'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Create a label table</h3></font>\n",
    "\n",
    "Try creating a different label table based on a slightly different definition of the label. How would you create the labels if you were interested in whether graduates went into a government job? What if you wanted a different cohort (for example, only looking at people in certain fields)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to subset by people who work in the U.S. government (federal, state, and local), we would specify: <br>`CASE WHEN pdemploy IN ('H','I','J')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label table\n",
    "sql = \"\"\"\n",
    "SELECT drf_id, phdinst,\n",
    "    CASE WHEN pdemploy IN ('H', 'I', 'J') THEN 1 ELSE 0 END as label  \n",
    "FROM ncses_2019.nsf_sed \n",
    "WHERE phdfy = '2015' \n",
    "AND \n",
    "phdinst in -- list IRIS member universities\n",
    "    (A list of IRIS Institution IDs which did not pass dislcosure review);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the SQL\n",
    "df = pd.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many people with label = 1 (work in the U.S. government)\n",
    "len(df[df['label'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem. \n",
    "\n",
    "- **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "\n",
    "Example of feature engineering are: \n",
    "\n",
    "- **Transformations**, such a log, square, and square root.\n",
    "- **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables\n",
    "(such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "- **Discretization**. Several methods require features to be discrete instead of continuous. This is often done \n",
    "by binning, which you can do by various approaches like equal width, deciles, Fisher-Jenks, etc. \n",
    "- **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use \n",
    "different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several\n",
    "values into one feature, aggregating over varying windows of time and space. For example, for policing or criminal justice problems, we may want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    ">This notebook walks through creating the following features:\n",
    ">- Individual-level funding history variables, taken from the UMETRICS.\n",
    ">- Demographic characteristics, taken from the SED.\n",
    ">- Institution-level characteristics from HERDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation Plan\n",
    "\n",
    "We will be creating a series of temporary tables containing all of the features we want to include in our model. These tables will be:\n",
    "- `features_fund`: This table will contain information about the funding history of the individual from UMETRICS.\n",
    "- `features_ind`: This table will contain demographic information about the individual such as race and sex.\n",
    "- `features_inst`: This table will contain institution level information from HERD.\n",
    "\n",
    "We will then join these tables together, along with our labels table, to create the full dataset that we use for our machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual level features\n",
    "\n",
    "We will start by creating a table containing all of the individual level features. We start by bringing in data from the UMETRICS files, then move on to the SED variables. For many variables, we can simply use them as they are. However, `sklearn`, which we will be using for our machine learning algorithms, we need to convert all categorical variables into binary variables (that is, make dummy variables). We can do that in Python, so it will be covered in the next notebook. In addition, we might need to consider some aggregating and reformatting in order to get the data that we want. This is most apparent in the UMETRICS data that we pull.\n",
    "\n",
    "> Note: The way we decide to bring in the semester level funding data is based on how we're choosing to define the features. This could be different for your problem! Additionally, the code to format it is just one way of doing it. If you think of a different way to do it that makes more intuitive sense to you, that might be even better!\n",
    "\n",
    "Let's start with the UMETRICS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMETRICS variables \n",
    "# Only getting whether a person was funded, as well as total semesters with any federal funding\n",
    "# You can add more!\n",
    "\n",
    "sql = '''\n",
    "drop table if exists semester_funding;\n",
    "create temp table semester_funding as\n",
    "select a.drf_id, a.phdinst, \n",
    "cast(case when team_size >= 1 then 1 else 0 end as int) as any_federal,\n",
    "cast(case when any_non_federal = 1 then 1 else 0 end as int) as any_non_federal,\n",
    "case when semester = '2012-jan-apr' then 1 else 0 end as spr12,\n",
    "case when semester = '2012-may-aug' then 1 else 0 end as sum12,\n",
    "case when semester = '2012-sep-dec' then 1 else 0 end as fal12,\n",
    "case when semester = '2013-jan-apr' then 1 else 0 end as spr13,\n",
    "case when semester = '2013-may-aug' then 1 else 0 end as sum13,\n",
    "case when semester = '2013-sep-dec' then 1 else 0 end as fal13,\n",
    "case when semester = '2014-jan-apr' then 1 else 0 end as spr14,\n",
    "case when semester = '2014-may-aug' then 1 else 0 end as sum14,\n",
    "case when semester = '2014-sep-dec' then 1 else 0 end as fal14\n",
    "from ada_ncses_2019.umetrics_label a\n",
    "left join ncses_2019.sed_umetrics_xwalk b\n",
    "on a.drf_id = b.drf_id\n",
    "left join ncses_2019.iris_semester c \n",
    "on b.emp_number = c.emp_number;\n",
    "\n",
    "drop table if exists features_fund;\n",
    "create temp table features_fund as\n",
    "select drf_id, \n",
    "sum(any_federal) as total_sem_federal,\n",
    "sum(any_non_federal) as total_sem_non_federal,\n",
    "sum(spr12) as spr12, sum(sum12) as sum12, sum(fal12) as fal12,\n",
    "sum(spr13) as spr13, sum(sum13) as sum13, sum(fal12) as fal13,\n",
    "sum(spr14) as spr14, sum(sum14) as sum14, sum(fal12) as fal14\n",
    "from semester_funding\n",
    "group by drf_id;\n",
    "'''\n",
    "conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `features_fund` table to make sure it's reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_fund', conn)\n",
    "df.shape # Check number of rows and columns - is this number correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of people who were federally funded on all semesters in which they were funded\n",
    "np.mean(df.iloc[:,2:].sum(axis = 1) == df.total_sem_federal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the individual-level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SED Individual variables\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_ind;\n",
    "CREATE TEMP TABLE features_ind AS \n",
    "SELECT cohort.drf_id, cohort.label, -- ID and Label\n",
    "phdfield_name, --phd info\n",
    "race, sex -- demographic variables \n",
    "FROM ada_ncses_2019.umetrics_label cohort\n",
    "LEFT JOIN ncses_2019.nsf_sed sed\n",
    "ON cohort.drf_id = sed.drf_id\n",
    "where phdfy = '2015';\n",
    "'''\n",
    "conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data to make sure it's working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_ind', conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in Institutional characteristics\n",
    "\n",
    "Now, we want to add in institutional characteristics from the HERD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in institutional level features from various sources\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS features_inst;\n",
    "CREATE TEMP TABLE features_inst AS\n",
    "SELECT cohort.drf_id, cohort.phdinst, \n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM ada_ncses_2019.umetrics_label cohort\n",
    "LEFT JOIN ncses_2019.nsf_herd herd\n",
    "ON cohort.phdinst = herd.ipeds_inst_id \n",
    "where herd.year = '2015' \n",
    "'''\n",
    "\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll do a little checking to make sure the variables we're bringing in look like they should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from features_inst', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the `isna()` method for DataFrames in order to check if there are missing values. We use it in conjunction with the `sum()` method to find how many missing values there are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all to make an overall table with labels and features\n",
    "\n",
    "Now that we've created all of our individual and institution level feature tables, we can combine them all into one big table that we will use for our machine learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS ada_ncses_2019.umetrics_ml_aggregate;\n",
    "CREATE TABLE ada_ncses_2019.umetrics_ml_aggregate AS\n",
    "SELECT ind.drf_id, label, -- ID and label\n",
    "total_sem_federal, total_sem_non_federal, spr12, sum12, fal12, spr13, sum13, fal13, spr14, sum14, fal14, -- UMETRICS data\n",
    "phdfield_name, --phd info\n",
    "race, sex, -- demographic variables \n",
    "hhe_flag, hbcu_flag, -- Flags for HBCU and HHE\n",
    "total_rd, federal_rd -- R&D Funding\n",
    "FROM features_ind ind\n",
    "LEFT JOIN features_fund fund\n",
    "ON ind.drf_id = fund.drf_id\n",
    "LEFT JOIN features_inst inst\n",
    "ON ind.drf_id = inst.drf_id\n",
    "'''\n",
    "\n",
    "if 'umetrics_ml_aggregate' in tables.tablename.tolist():\n",
    "    print('Table already created.')\n",
    "else:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from ada_ncses_2019.umetrics_ml_aggregate', conn)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3>Checkpoint 2: Create a Feature and create an overall table</h3></font>\n",
    "\n",
    "What are some additional features you might want to add? Think about the different variables in all of the different tables that you have access to, both at the institutional level and at the individual level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Considerations\n",
    "\n",
    "Notice that there are missing values in the final table we created. We'll have to figure out how to deal with those. By default, if we try to run our machine learning models using scikit-learn, it will use listwise deletion, which is not always desirable. In addition, we should carefully consider whether there are any other errors in our dataset. For example, there might have been data entry errors, or coding mistakes when transferring the data.\n",
    "\n",
    "### Removing Outliers \n",
    "\n",
    "**It is never a good idea to drop observations without prior investigation AND a good reason to believe the data is wrong!** \n",
    "\n",
    "### Imputing Missing Values\n",
    "\n",
    "There are many ways of imputing missing values based on the rest of the data. Missing values can be imputed to median of the rest of the data, or you can use other characteristics (eg industry, geography, etc.).\n",
    "\n",
    "For our data, we have made an assumption about what \"missing\" means for each of our data's components (eg if the individual does not show up in the IDES data we say they do not have a job in that time period).\n",
    "\n",
    "Before running any machine learning algorithms, we have to ensure there are no `NULL` (or `NaN`) values in the data for both our testing and training sets. As you have heard before, __never remove observations with missing values without considering the data you are dropping__. One easy way to check if there are any missing values with `Pandas` is to use the `.info()` method, which returns a count of non-null values for each column in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
